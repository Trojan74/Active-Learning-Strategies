# -*- coding: utf-8 -*-
"""CNN_Active_Learning_Project_ver_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LuE27El05owmTuZPIsBzNRLNunOUlHHc

# SETUP
"""

#!pip install laplace-torch --upgrade

"""### Import all dependencies"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import torchvision
import torchvision.transforms as transforms
from torchvision.utils import make_grid
from sklearn import metrics
from sklearn.model_selection import train_test_split

sns.set_theme(style="whitegrid")

"""### import CIFAR-10"""

# 1. Download CIFAR-10
cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)
cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)

# 2. Access data and targets
X_train = np.array([np.array(x) for x, y in cifar10_train])
y_train = np.array([y for x, y in cifar10_train])

X_test = np.array([np.array(x) for x, y in cifar10_test])
y_test = np.array([y for x, y in cifar10_test])

# Map from class index to class name.
classes = {index: name for name, index in cifar10_train.class_to_idx.items()}

def plot_random_examples(X, y, num_examples=5):
    """
    Plots a given number of random examples from the dataset.
    """
    num_images = X.shape[0]

    plt.figure(figsize=(num_examples * 2, 2))
    random_indices = np.random.choice(num_images, num_examples, replace=False)

    for i, idx in enumerate(random_indices):
        image = X[idx].reshape(32, 32, 3)/255 # Reshape for CIFAR-10
        plt.subplot(1, num_examples, i + 1)
        plt.imshow(image/2.1295387686331444)
        plt.axis('off')
        plt.title(f"idx = {idx},   y = {y[idx]}")

    plt.tight_layout()
    plt.show()

plot_random_examples(X_train, y_train, 6)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)
print(classes)

# 3. Split data
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

"""### Normalize X"""

mean = np.mean(X_train, axis=(0, 1, 2))
std = np.std(X_train, axis=(0, 1, 2))

X_train = (X_train - mean) / std
X_val = (X_val - mean) / std
X_test = (X_test - mean) / std

print("X_train min/max:", X_train.min(), X_train.max())
print("X_val min/max:", X_val.min(), X_val.max())
print("X_test min/max:", X_test.min(), X_test.max())

"""### Define Classification Class"""

import torch.nn.init as init
from copy import deepcopy

class CIFAR10ClassifierModule(nn.Module):
    def __init__(self, num_classes=10):
        super(CIFAR10ClassifierModule, self).__init__()

        # Convolutional layer dimensions
        self.dim1 = 20
        self.dim2 = 48
        self.dim3 = 64

        # Convolutional Layers
        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=self.dim1, kernel_size=3, stride=1, padding=1)
        self.batchnorm1 = nn.BatchNorm2d(self.dim1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv_layer2 = nn.Conv2d(in_channels=self.dim1, out_channels=self.dim2, kernel_size=3, stride=1, padding=1)
        self.batchnorm2 = nn.BatchNorm2d(self.dim2)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv_layer3 = nn.Conv2d(in_channels=self.dim2, out_channels=self.dim3, kernel_size=3, stride=1, padding=1)
        self.batchnorm3 = nn.BatchNorm2d(self.dim3)
        self.relu3 = nn.ReLU()
        self.pool3 = nn.MaxPool2d(kernel_size=4, stride=4)

        # Fully Connected Layers
        self.fc_layer1 = nn.Linear(self.dim3*4, self.dim3*8)      # 512 -> 64
        self.relu5 = nn.ReLU()
        self.dropout1 = nn.Dropout(p=0.3)

        self.fc_layer2 = nn.Linear(self.dim3*8, self.dim3)  # 64 -> 32
        self.relu6 = nn.ReLU()
        self.dropout2 = nn.Dropout(p=0.3)

        self.fc_layer3 = nn.Linear(self.dim3, num_classes)   # 32 -> 10

        # Initialize weights
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            init.kaiming_normal_(module.weight, nonlinearity='relu')
            if module.bias is not None:
                init.constant_(module.bias, 0)
        # Save the initial parameters after weights are initialized
        self.init_params = deepcopy(self.state_dict())

    def compute_features(self,X):
        self.eval()
        with torch.no_grad():
            X = self.conv_layer1(X)
            X = self.batchnorm1(X)
            X = self.relu1(X)
            X = self.pool1(X)


            X = self.conv_layer2(X)
            X = self.batchnorm2(X)
            X = self.relu2(X)
            X = self.pool2(X)


            X = self.conv_layer3(X)
            X = self.batchnorm3(X)
            X = self.relu3(X)
            X = self.pool3(X)
            return X.view(X.size(0), -1)
            #X = self.global_avg_pool(X)

        return X.squeeze(-1).squeeze(-1)


    def forward(self, X, **kwargs):
        feature_maps = {}

        # Convolutional Layer 1
        X = self.conv_layer1(X)
        X = self.batchnorm1(X)
        X = self.relu1(X)
        feature_maps['conv1'] = X
        X = self.pool1(X)

        # Convolutional Layer 2
        X = self.conv_layer2(X)
        X = self.batchnorm2(X)
        X = self.relu2(X)
        feature_maps['conv2'] = X
        X = self.pool2(X)

        # Convolutional Layer 3
        X = self.conv_layer3(X)
        X = self.batchnorm3(X)
        X = self.relu3(X)
        feature_maps['conv3'] = X
        X = self.pool3(X)

        # Global Average Pooling
        #X = self.global_avg_pool(X)  # Shape: (batch_size, dim4, 1, 1)
        X = X.view(X.size(0), -1)    # Shape: (batch_size, dim4)
        #feature_maps['final'] = X

        # Fully Connected Layer 1
        X = self.fc_layer1(X)
        X = self.relu5(X)
        X = self.dropout1(X)

        # Fully Connected Layer 2 (New)
        X = self.fc_layer2(X)
        X = self.relu6(X)
        X = self.dropout2(X)

        # Fully Connected Layer 3 (Output)
        X = self.fc_layer3(X)

        if kwargs.get("laplace_context", False):  # Check for laplace_context flag
            return X  # Return only the output
        else:
            return X, feature_maps  # Return output and feature maps as usual

    def reset(self):
        self.load_state_dict(self.init_params)

    def save_checkpoint(self, optimizer, epoch, path):
        """
        Saves the model and optimizer states to the specified path.
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
        }
        torch.save(checkpoint, path)
        print(f"Model and optimizer states saved to {path}")

    def load_checkpoint(self, optimizer, path, device):
        """
        Loads the model and optimizer states from the specified path.
        """
        checkpoint = torch.load(path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        epoch = checkpoint['epoch']
        print(f"Model and optimizer states loaded from {path} (Epoch {epoch})")
        return epoch

# Instantiate and print the model
model = CIFAR10ClassifierModule()
print(model)

def count_parameters(model):
    """
    Counts and prints the number of parameters in each layer of the model,
    as well as the total number of parameters.
    """
    total_params = 0
    layer_params = {}

    print("Layer-wise Parameter Count:\n" + "-"*30)
    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.BatchNorm2d, nn.Linear)):
            params = sum(p.numel() for p in module.parameters())
            layer_params[name] = params
            total_params += params
            print(f"{name:<20} | {params:<10}")

    print("-"*30)
    print(f"{'Total Parameters':<20} | {total_params}")

# Call the function to display parameter counts
count_parameters(model)

def check_model_grad_flow(model, input_shape=(3, 32, 32), num_classes=10):
    """
    Checks a PyTorch model for gradient-related issues and potential clashes with higher-order derivatives.

    Args:
        model: The PyTorch model to check.
        input_shape: Tuple, shape of the input tensor.
        num_classes: Integer, number of classes for the output (default: 10 for CIFAR-10).
    """
    print("=== Model Check Report ===")
    print("\n1. Checking if all parameters require gradients:")
    for name, param in model.named_parameters():
        print(f"{name}: requires_grad={param.requires_grad}")

    print("\n2. Running a forward pass...")
    try:
        inputs = torch.randn(2, *input_shape)  # Batch size of 2
        outputs, _ = model(inputs)
        if outputs.size(1) != num_classes:
            print(f"[WARNING] Output size mismatch: expected {num_classes} classes, got {outputs.size(1)}.")
        print("[SUCCESS] Forward pass completed.")
    except Exception as e:
        print(f"[ERROR] Forward pass failed: {e}")
        return

    print("\n3. Checking first-order gradients...")
    try:
        labels = torch.tensor([0, 1])  # Dummy labels
        loss = nn.CrossEntropyLoss()(outputs, labels)
        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)
        for i, grad in enumerate(grads):
            if grad is None or not grad.requires_grad:
                print(f"[WARNING] Gradient issue for parameter {list(model.parameters())[i].shape}.")
        print("[SUCCESS] First-order gradients computed.")
    except Exception as e:
        print(f"[ERROR] First-order gradient computation failed: {e}")
        return

    print("\n4. Checking second-order gradients...")
    try:
        for param in model.parameters():
            if param.requires_grad:
                grad = torch.autograd.grad(loss, param, create_graph=True, retain_graph=True)[0]
                if grad is not None:
                    grad2 = torch.autograd.grad(grad, param, grad_outputs=torch.ones_like(grad), retain_graph=True)[0]
                    if grad2 is None:
                        print(f"[WARNING] Second-order gradient issue for parameter {param.shape}.")
        print("[SUCCESS] Second-order gradients computed.")
    except Exception as e:
        print(f"[ERROR] Second-order gradient computation failed: {e}")

    print("\n5. Checking BatchNorm and ReLU layers...")
    for name, module in model.named_modules():
        if isinstance(module, nn.BatchNorm2d):
            print(f"BatchNorm layer '{name}': affine={module.affine}, track_running_stats={module.track_running_stats}")
        elif isinstance(module, nn.ReLU):
            print(f"ReLU layer '{name}': inplace={module.inplace}")

    print("\n6. Checking custom initialization...")
    try:
        model.apply(model._init_weights)
        print("[SUCCESS] Custom initialization passed.")
    except Exception as e:
        print(f"[ERROR] Custom initialization failed: {e}")

    print("\n=== End of Report ===")


# Example Usage
model = CIFAR10ClassifierModule()
check_model_grad_flow(model)

from typing import Tuple, List

class Trainer:
    def __init__(
        self,
        model: nn.Module,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        X_test: np.ndarray,
        y_test: np.ndarray,
        batch_size: int = 128,
        learning_rate: float = 0.00005,
        weight_decay: float = 5e-3,
        num_epochs: int = 15,
        validation_every_steps: int = 100,
        device: str = None,
    ):
        # Device configuration
        self.device = torch.device(device if device else ("cuda" if torch.cuda.is_available() else "cpu"))

        # Prepare data loaders
        self.train_loader, self.val_loader, self.test_loader = self._prepare_data_loaders(
            X_train, y_train, X_val, y_val, X_test, y_test, batch_size
        )

        # Model
        self.model = model.to(self.device)

        # Loss and optimizer
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = torch.optim.Adam(
            self.model.parameters(), lr=learning_rate, weight_decay=weight_decay
        )

        # Training parameters
        self.num_epochs = num_epochs
        self.validation_every_steps = validation_every_steps
        self.step = 0

        # Metrics
        self.train_losses: List[float] = []
        self.val_losses: List[float] = []
        self.train_accuracies: List[float] = []
        self.val_accuracies: List[float] = []

    def _prepare_data_loaders(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        X_test: np.ndarray,
        y_test: np.ndarray,
        batch_size: int,
    ) -> Tuple[DataLoader, DataLoader, DataLoader]:
        # Convert numpy arrays to PyTorch tensors
        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).permute(0, 3, 1, 2)
        y_train_tensor = torch.tensor(y_train, dtype=torch.long)
        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).permute(0, 3, 1, 2)
        y_val_tensor = torch.tensor(y_val, dtype=torch.long)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).permute(0, 3, 1, 2)
        y_test_tensor = torch.tensor(y_test, dtype=torch.long)

        # Create TensorDatasets
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

        # Create DataLoaders
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        return train_loader, val_loader, test_loader

    @staticmethod
    def accuracy(targets: torch.Tensor, predictions: torch.Tensor) -> float:
        return (targets == predictions).sum().item() / targets.size(0)

    def evaluate(self, loader: DataLoader) -> Tuple[float, float]:
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, targets in loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                outputs = self.model(inputs)[0]
                loss = self.criterion(outputs, targets)
                total_loss += loss.item() * inputs.size(0)

                predictions = outputs.argmax(dim=1)
                correct += (predictions == targets).sum().item()
                total += targets.size(0)

        average_loss = total_loss / total
        accuracy_score = correct / total
        self.model.train()
        return average_loss, accuracy_score

    def train(self):
        self.model.train()

        for epoch in range(1, self.num_epochs + 1):
            print(f"Epoch {epoch}/{self.num_epochs}")
            for inputs, targets in self.train_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)

                # Forward pass
                outputs = self.model(inputs)[0]
                loss = self.criterion(outputs, targets)

                # Backward pass and optimization
                loss.backward()
                self.optimizer.step()
                self.optimizer.zero_grad()

                # Metrics
                self.train_losses.append(loss.item())
                predictions = outputs.argmax(dim=1)
                acc = self.accuracy(targets, predictions)
                self.train_accuracies.append(acc)

                self.step += 1

                # Validation
                if self.step % self.validation_every_steps == 0:
                    val_loss, val_acc = self.evaluate(self.val_loader)
                    self.val_losses.append(val_loss)
                    self.val_accuracies.append(val_acc)

                    print(f"Step {self.step:<5} Training Loss: {loss.item():.4f} | Training Acc: {acc:.4f}")
                    print(f"           Validation Loss: {val_loss:.4f} | Validation Acc: {val_acc:.4f}")

            # End of epoch validation
            val_loss, val_acc = self.evaluate(self.val_loader)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_acc)

            print(f"End of Epoch {epoch} | Validation Loss: {val_loss:.4f} | Validation Acc: {val_acc:.4f}")

        print("Training complete.")

    def test(self):
        test_loss, test_acc = self.evaluate(self.test_loader)
        print(f"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}")
        return test_loss, test_acc

    def plot_metrics(self):
        epochs = range(1, self.num_epochs + 1)

        plt.figure(figsize=(12, 5))

        # Plot Loss
        plt.subplot(1, 2, 1)
        plt.plot(self.train_losses, label='Training Loss')
        plt.plot(
            np.linspace(0, len(self.train_losses), len(self.val_losses)),
            self.val_losses,
            label='Validation Loss'
        )
        plt.xlabel('Steps')
        plt.ylabel('Loss')
        plt.title('Loss over Steps')
        plt.legend()

        # Plot Accuracy
        plt.subplot(1, 2, 2)
        plt.plot(self.train_accuracies, label='Training Accuracy')
        plt.plot(
            np.linspace(0, len(self.train_accuracies), len(self.val_accuracies)),
            self.val_accuracies,
            label='Validation Accuracy'
        )
        plt.xlabel('Steps')
        plt.ylabel('Accuracy')
        plt.title('Accuracy over Steps')
        plt.legend()

        plt.tight_layout()
        plt.show()

# Usage Example

# Assume `model`, `X_train`, `y_train`, `X_val`, `y_val`, `X_test`, `y_test` are predefined
# model = YourModelDefinition()

# Initialize the Trainer
trainer = Trainer(
    model=model,
    X_train=X_train,
    y_train=y_train,
    X_val=X_val,
    y_val=y_val,
    X_test=X_test,
    y_test=y_test,
    batch_size=64,
    learning_rate=0.0005,
    weight_decay=1e-4,
    num_epochs=15,
    validation_every_steps=100
)

# Start training
trainer.train()

# Evaluate on the test set
trainer.test()

# Plot training and validation metrics
trainer.plot_metrics()

"""# ACTIVE LEARNING TRAINER

### Class Initialization

We generate an unlabeled dataset and a labeled dataset:
"""

from torch.utils.data import random_split, TensorDataset

# Define initial labeled and unlabeled datasets
initial_train_size = int(len(X_train)*0.20) # Small initial training set
val_size = int(len(X_train) * 0.05) # Validation set size
unlabeled_size = len(X_train) - initial_train_size - val_size

# Convert CIFAR-10 numpy arrays to TensorDatasets
train_dataset = TensorDataset(torch.tensor(X_train).permute(0, 3, 1, 2).float(), torch.tensor(y_train))
test_dataset = TensorDataset(torch.tensor(X_test).permute(0, 3, 1, 2).float(), torch.tensor(y_test))

# Split CIFAR-10 into labeled training, validation, and unlabeled sets
train_data, val_data, unlabeled_data = random_split(train_dataset, [initial_train_size, val_size, unlabeled_size])

"""# ACTIVE LEARNER WITH LAPLACE"""

from torch.utils.data import Subset
import collections

class ActiveLearningTrainer:
    def __init__(self, model, criterion, optimizer, test_data, train_data, val_data, unlabeled_data,
                 subset_size=2000, batch_size=128, device=None, posterior="plugin",
                 sampling_strategy="entropy", num_cycles=5, epochs_per_cycle=10, num_samples_per_cycle=100,
                 convergence_threshold=1e-3, convergence_epochs=20, alpha=1, init_epochs=50, pretrain=True,
                 epoch_print=True):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.train_data = train_data
        self.val_data = val_data
        self.unlabeled_data = unlabeled_data
        self.test_data = test_data
        self.train_loader = DataLoader(self.train_data, batch_size=batch_size, shuffle=True)
        self.val_loader = DataLoader(self.val_data, batch_size=batch_size, shuffle=False)
        self.unlabeled_loader = DataLoader(self.unlabeled_data, batch_size=batch_size, shuffle=False)
        self.test_loader = DataLoader(self.test_data, batch_size=batch_size, shuffle=False)
        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.posterior = posterior
        self.sampling_strategy = sampling_strategy
        if posterior == "laplace":
            self.laplace_approximation = LaplaceApproximation(self)
        else:
            self.laplace_approximation = None

        self.subset_size = subset_size
        self.init_epochs = init_epochs

        self.num_cycles = num_cycles
        self.epochs_per_cycle = epochs_per_cycle
        self.num_samples_per_cycle = num_samples_per_cycle

        self.convergence_epochs = convergence_epochs

        self.alpha = alpha

        self.pretrain = pretrain

        self.epoch_print=epoch_print


        self.model.to(self.device)

    def compute_output(self, inputs):
        if self.posterior == "plugin":
            outputs, _ = self.model(inputs)
            probabilities = F.softmax(outputs, dim=1)
        elif self.posterior == "laplace":
            probabilities = self.laplace_approximation.bayesian_output(inputs, N=30)
        else:
            raise ValueError(f"Unsupported posterior method: {self.posterior}")
        return probabilities

    def entropy_score(self, inputs):
        """
        Compute entropy of the model's outputs to measure uncertainty.
        """
        prob = self.compute_output(inputs)
        entropy = -torch.sum(prob * torch.log(prob + 1e-16), dim=1)
        return entropy

    def redundancy_score(self, inputs):
        features = self.model.compute_features(inputs)

        features = torch.nn.functional.normalize(features, p=2, dim=1)

        similarity_matrix = torch.mm(features, features.T)
        similarity_matrix.fill_diagonal_(0)

        redundancy_scores = torch.sum(similarity_matrix, dim=1)

        return redundancy_scores

    def select_samples(self, num_samples):
        """Select samples based on the specified sampling strategy."""
        subset_size = self.subset_size
        if subset_size > len(self.unlabeled_data):
            subset_size = len(self.unlabeled_data)

        if self.sampling_strategy == "entropy":
            # Select samples with the highest entropy from a subset of the unlabeled data
            self.model.eval()
            entropies = []
            indices = []

            subset_size = min(subset_size, len(self.unlabeled_data))
            subset_indices = np.random.choice(len(self.unlabeled_data), size=subset_size, replace=False)
            subset_loader = DataLoader(Subset(self.unlabeled_data, subset_indices),
                                      batch_size=self.unlabeled_loader.batch_size, shuffle=False)

            with torch.no_grad():
                for idx, (inputs, _) in enumerate(subset_loader):
                    inputs = inputs.to(self.device)
                    entropy = self.entropy_score(inputs)
                    entropies.extend(entropy.cpu().numpy())
                    indices.extend([subset_indices[idx * subset_loader.batch_size + i]
                                    for i in range(len(inputs))])

            selected_indices = np.argsort(entropies)[-num_samples:]
            return [indices[i] for i in selected_indices]

        elif self.sampling_strategy == "entropy-redundancy":
            self.model.eval()
            entropies = []
            indices = []

            subset_size = min(subset_size, len(self.unlabeled_data))
            subset_indices = np.random.choice(len(self.unlabeled_data), size=subset_size, replace=False)
            subset_loader = DataLoader(Subset(self.unlabeled_data, subset_indices),
                                      batch_size=self.unlabeled_loader.batch_size, shuffle=False)

            with torch.no_grad():
                for idx, (inputs, _) in enumerate(subset_loader):
                    inputs = inputs.to(self.device)

                    entropy = self.entropy_score(inputs)
                    redundancy = self.redundancy_score(inputs)

                    combined_score = entropy - (self.alpha / len(subset_indices)) * redundancy

                    entropies.extend(combined_score.cpu().numpy())
                    indices.extend([subset_indices[idx * subset_loader.batch_size + i]
                                    for i in range(len(inputs))])

            selected_indices = np.argsort(entropies)[-num_samples:]
            return [indices[i] for i in selected_indices]

        elif self.sampling_strategy == "random":
            # Randomly select samples from the unlabeled data
            remaining_indices = list(range(len(self.unlabeled_data)))
            if num_samples >= len(remaining_indices):
                selected_indices = remaining_indices
            else:
                selected_indices = np.random.choice(remaining_indices, size=num_samples, replace=False)
            return selected_indices.tolist()
        else:
            raise ValueError(f"Unsupported sampling strategy: {self.sampling_strategy}")


    def add_to_train_set(self, selected_indices):
        """Add selected samples to the training set."""
        new_train_indices = list(range(len(self.train_data))) + selected_indices
        self.train_data = Subset(self.train_data + self.unlabeled_data, new_train_indices)
        self.train_loader = DataLoader(self.train_data, batch_size=self.train_loader.batch_size, shuffle=True)

    def train_epoch(self):
        """Train for one epoch with the current training data."""
        self.model.train()
        running_loss = 0.0

        for inputs, labels in self.train_loader:
            inputs, labels = inputs.to(self.device), labels.to(self.device).long()

            self.optimizer.zero_grad()
            outputs, _ = self.model(inputs)
            loss = self.criterion(outputs, labels)
            loss.backward()
            self.optimizer.step()

            running_loss += loss.item() * inputs.size(0)

        epoch_loss = running_loss / len(self.train_loader.dataset)
        return epoch_loss

    def validate(self):
        self.model.eval()
        running_loss = 0.0
        running_corrects = 0
        total_samples = 0

        with torch.no_grad():
            for inputs, labels in self.val_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device).long()
                outputs, _ = self.model(inputs)
                loss = self.criterion(outputs, labels)
                running_loss += loss.item() * inputs.size(0)

                _, preds = torch.max(outputs, 1)
                running_corrects += (preds == labels).sum().item()
                total_samples += labels.size(0)

        avg_loss = running_loss / total_samples
        accuracy = running_corrects / total_samples
        return avg_loss, accuracy

    def compute_test_metrics(self):
        self.model.eval()

        correct_predictions, total_samples = 0, 0
        total_loss = 0

        with torch.no_grad():
            for inputs, labels in self.test_loader:  # Use your pre-defined test_loader
                inputs, labels = inputs.to(self.device), labels.to(self.device).long()
                outputs, _ = self.model(inputs)

                # Compute loss
                total_loss += self.criterion(outputs, labels).item()

                # Compute predictions
                _, preds = torch.max(outputs, 1)
                correct_predictions += (preds == labels).sum().item()
                total_samples += labels.size(0)

        accuracy = correct_predictions / total_samples
        average_loss = total_loss / len(self.test_loader)
        return average_loss, accuracy

    def active_learning_training(self):
        """Main training loop with active learning."""
        metrics = {
            "cycle": [],
            "train_loss": [],
            "val_loss": [],
            "val_accuracy": [],
            "train_sample_size": [],
            "test_accuracy": [],
            "test_loss": [],
            "average_entropy": []
        }

        print("Evaluating initial metrics...")
        initial_val_loss, initial_val_accuracy = self.validate()
        print(f"Initial Validation Loss: {initial_val_loss:.4f}, Accuracy: {initial_val_accuracy:.4f}\n")

        # Calculate total epochs for logging purposes
        pretrain_epochs = self.init_epochs if self.pretrain else 0
        total_epochs = pretrain_epochs + (self.num_cycles * self.epochs_per_cycle)
        print("Max Scheduled Epochs: ", total_epochs)
        current_epoch = 0

        # **Pre-training Phase:** Only if pretrain is True
        if self.pretrain:
            print("Starting Pre-training Phase...")

            recent_train_losses = collections.deque(maxlen=self.convergence_epochs)
            recent_val_losses = collections.deque(maxlen=self.convergence_epochs)

            lowest_train_loss = float('inf')
            lowest_val_loss = float('inf')

            for epoch in range(self.init_epochs):
                current_epoch += 1
                print(f"  Pre-training Epoch {epoch + 1}/{self.init_epochs}")

                # Train
                train_loss = self.train_epoch()
                print(f"    Training Loss: {train_loss:.4f}")

                # Update Laplace (if applicable)
                if self.laplace_approximation:
                    self.laplace_approximation.update()

                # Validate
                val_loss, val_accuracy = self.validate()
                print(f"    Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}")

                # Update recent losses for convergence check
                recent_train_losses.append(train_loss)
                recent_val_losses.append(val_loss)

                # Check for convergence
                if (len(recent_train_losses) == self.convergence_epochs and
                    len(recent_val_losses) == self.convergence_epochs):

                    # Determine if training has converged
                    train_converged = min(recent_train_losses) > lowest_train_loss
                    val_converged = min(recent_val_losses) > lowest_val_loss
                    if train_converged and val_converged:
                        print(f"  Convergence reached!")
                        break

                # Update lowest losses
                if train_loss < lowest_train_loss:
                    lowest_train_loss = train_loss
                if val_loss < lowest_val_loss:
                    lowest_val_loss = val_loss

                # Record overall training metrics

            # Compute and store test metrics after pre-training
            test_loss, test_accuracy = self.compute_test_metrics()
            print(f"Test Loss after Pre-training: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")

            # Compute average entropy after pre-training
            entropy_scores = []
            self.model.eval()
            with torch.no_grad():
                for inputs, _ in self.val_loader:
                    inputs = inputs.to(self.device)
                    entropy = self.entropy_score(inputs)
                    entropy_scores.extend(entropy.cpu().numpy())
            average_entropy = float(np.mean(entropy_scores))
            print(f"Average Entropy after Pre-training: {average_entropy:.4f}\n")

        for cycle in range(1, self.num_cycles + 1):
            print(f"\nCycle {cycle}/{self.num_cycles}")

            # **Sample Acquisition Phase:** Before the start of the cycle
            print(f"\nSelecting {self.num_samples_per_cycle} Samples - Sampling Strategy: '{self.sampling_strategy}'")
            selected_indices = self.select_samples(self.num_samples_per_cycle)
            self.add_to_train_set(selected_indices)
            print(f"Current training set size: {len(self.train_data)}")

            # Update unlabeled data by removing selected indices
            remaining_unlabeled_indices = set(range(len(self.unlabeled_data))) - set(selected_indices)
            self.unlabeled_data = Subset(self.unlabeled_data, list(remaining_unlabeled_indices))

            # Update the DataLoader for unlabeled data
            self.unlabeled_loader = DataLoader(
                self.unlabeled_data,
                batch_size=self.unlabeled_loader.batch_size,
                shuffle=False
            )

            # Check if unlabeled data is exhausted
            if len(self.unlabeled_data) == 0:
                print("No unlabeled data left. Ending active learning early.")
                break

            # Record metrics for the current cycle
            metrics["train_sample_size"].append(len(self.train_data))
            metrics["cycle"].append(cycle)

            # Reset metrics for convergence detection
            recent_train_losses = collections.deque(maxlen=self.convergence_epochs)
            recent_val_losses = collections.deque(maxlen=self.convergence_epochs)

            lowest_train_loss = float('inf')
            lowest_val_loss = float('inf')

            # Training phase for the cycle
            for epoch in range(self.epochs_per_cycle):
                current_epoch += 1


                # Train
                train_loss = self.train_epoch()

                # Update Laplace (if applicable)
                if self.laplace_approximation:
                    self.laplace_approximation.update()

                # Validate
                val_loss, val_accuracy = self.validate()
                if self.epoch_print == True:
                  print(f"  Epoch {epoch + 1}/{self.epochs_per_cycle}")
                  print(f"    Training Loss: {train_loss:.4f}")
                  print(f"    Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}")

                # Update recent losses for convergence check
                recent_train_losses.append(train_loss)
                recent_val_losses.append(val_loss)

                # Check for convergence
                if (len(recent_train_losses) == self.convergence_epochs and
                    len(recent_val_losses) == self.convergence_epochs):

                    # Determine if training has converged
                    train_converged = min(recent_train_losses) > lowest_train_loss
                    val_converged = min(recent_val_losses) > lowest_val_loss
                    if train_converged and val_converged:
                        print(f"  Convergence reached!")
                        break

                # Update lowest losses
                if train_loss < lowest_train_loss:
                    lowest_train_loss = train_loss
                if val_loss < lowest_val_loss:
                    lowest_val_loss = val_loss

            # After training epochs for the cycle, record the last epoch's loss and accuracy
            metrics["train_loss"].append(train_loss)
            metrics["val_loss"].append(val_loss)
            metrics["val_accuracy"].append(val_accuracy)

            # Compute and store test metrics
            test_loss, test_accuracy = self.compute_test_metrics()
            metrics["test_accuracy"].append(test_accuracy)
            metrics["test_loss"].append(test_loss)
            print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")

            # Compute average entropy for the current cycle
            entropy_scores = []
            self.model.eval()
            with torch.no_grad():
                for inputs, _ in self.val_loader:
                    inputs = inputs.to(self.device)
                    entropy = self.entropy_score(inputs)
                    entropy_scores.extend(entropy.cpu().numpy())
            average_entropy = float(np.mean(entropy_scores))
            metrics["average_entropy"].append(average_entropy)
            print(f"Average Entropy: {average_entropy:.4f}")

            # Check if unlabeled data is exhausted
            if len(self.unlabeled_data) == 0:
                print("No unlabeled data left. Ending active learning early.")
                break

        print("Training Finalized!\n")
        return metrics

"""# LAPLACE APPROXIMATION"""

class LaplaceApproximation:
    def __init__(self, trainer):
        self.trainer = trainer
        self.model = trainer.model
        self.criterion = trainer.criterion
        self.device = trainer.device
        self.update()  # Initial update

    def update(self):
        # Update the MAP estimate
        map_params = []
        for param in self.model.parameters():
            map_params.append(param.view(-1))
        self.map = torch.cat(map_params)

        # Compute Hessian diagonal over the training data
        hessian_diag = None
        for inputs, labels in self.trainer.train_loader:
            diag = self.compute_hessian_diag(inputs, labels)
            if hessian_diag is None:
                hessian_diag = diag
            else:
                hessian_diag += diag
        hessian_diag /= len(self.trainer.train_loader)
        self.hessian_diag = hessian_diag

    def compute_hessian_diag(self, inputs, labels):
        # Ensure gradient computation is enabled within the function
        with torch.set_grad_enabled(True):
            self.model.train()
            inputs, labels = inputs.to(self.device), labels.to(self.device).long()
            outputs, _ = self.model(inputs)
            loss = self.criterion(outputs, labels)

            hessian_diag = []

            for param in self.model.parameters():
                # First derivative
                grad = torch.autograd.grad(loss, param, create_graph=True, retain_graph=True)[0]
                # Second derivative
                grad2 = torch.autograd.grad(grad, param, grad_outputs=torch.ones_like(grad), retain_graph=True)[0]
                hessian_diag.append(grad2.view(-1))

            return torch.cat(hessian_diag)

    def get_laplace_dist(self, inputs, labels):
        hessian_diag = self.hessian_diag

        damping = 1e-3
        hessian_diag_damped = hessian_diag + damping

        min_hessian_value = 1e-16
        hessian_diag_damped = torch.clamp(hessian_diag_damped, min=min_hessian_value)

        variance_diag = torch.reciprocal(hessian_diag_damped)

        max_variance = 1e6
        variance_diag = torch.clamp(variance_diag, max=max_variance)

        dist = torch.distributions.Independent(
            torch.distributions.Normal(self.map, variance_diag.sqrt()),
            reinterpreted_batch_ndims=1
        )
        return dist

    def bayesian_output(self, inputs, N=100):
        inputs = inputs.to(self.device)

        dist = self.get_laplace_dist(inputs, torch.zeros(inputs.size(0), device=self.device))

        # Compute the MAP output
        self._assign_parameters(self.map)
        map_output, _ = self.model(inputs)

        thetas = [dist.sample() for _ in range(N - 1)]
        thetas.append(self.map)

        probabilities = None
        for theta in thetas:
            self._assign_parameters(theta)
            output, _ = self.model(inputs)
            probs = F.softmax(output, dim=1)

            if probabilities is None:
                probabilities = probs
            else:
                probabilities += probs

        posterior_output = probabilities / N

        self._assign_parameters(self.map)

        #print(posterior_output,map_output)

        return posterior_output

    def _assign_parameters(self, theta):
        pointer = 0
        for param in self.model.parameters():
            num_params = param.numel()
            param.data.copy_(theta[pointer:pointer + num_params].view_as(param))
            pointer += num_params

"""# TRAINING MODELS"""

import torch
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

def train_initial(model, optimizer, criterion, train_data, val_data, test_data, init_epochs, batch_size, device):
    """
    Trains the model for a specified number of initial epochs, evaluates on validation data, and plots the metrics.
    """
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

    train_losses, train_accuracies = [], []
    val_losses, val_accuracies = [], []

    for epoch in range(init_epochs):
        model.train()
        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device).long()

            optimizer.zero_grad()
            outputs, _ = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            running_corrects += (preds == labels).sum().item()

        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_accuracy = running_corrects / len(train_loader.dataset)
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_accuracy)

        # Validation Phase
        model.eval()
        running_val_loss = 0.0
        running_val_corrects = 0

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device).long()
                outputs, _ = model(inputs)
                loss = criterion(outputs, labels)
                running_val_loss += loss.item() * inputs.size(0)

                _, preds = torch.max(outputs, 1)
                running_val_corrects += (preds == labels).sum().item()

        val_loss = running_val_loss / len(val_loader.dataset)
        val_accuracy = running_val_corrects / len(val_loader.dataset)
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        print(f"Epoch {epoch+1}/{init_epochs} - "
              f"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_accuracy:.4f} - "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}")

    print("\nInitial Training Completed!\n")

    # Test Phase
    running_test_loss = 0.0
    running_test_corrects = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device).long()
            outputs, _ = model(inputs)
            loss = criterion(outputs, labels)
            running_test_loss += loss.item() * inputs.size(0)

            _, preds = torch.max(outputs, 1)
            running_test_corrects += (preds == labels).sum().item()

    test_loss = running_test_loss / len(test_loader.dataset)
    test_accuracy = running_test_corrects / len(test_loader.dataset)

    print(f"Test Results - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\n")

    # Plotting Losses
    plt.figure(figsize=(8, 5),dpi=100)
    plt.plot(range(1, init_epochs + 1), train_losses, label='Train Loss')
    plt.plot(range(1, init_epochs + 1), val_losses, label='Val Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Losses over Epochs')
    plt.legend()
    plt.show()

    # Plotting Accuracies
    plt.figure(figsize=(8, 5),dpi=100)
    plt.plot(range(1, init_epochs + 1), train_accuracies, label='Train Accuracy')
    plt.plot(range(1, init_epochs + 1), val_accuracies, label='Val Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Accuracies over Epochs')
    plt.legend()
    plt.show()

    return model

# @title Compute alpha values
model = CIFAR10ClassifierModule(num_classes=10)
criterion = nn.CrossEntropyLoss()
learning_rate = 0.0001
weight_decay = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Active Learning Parameters
num_cycles = 1
init_epochs = 50
epochs_per_cycle = 30
num_samples_per_cycle = int(initial_train_size*0.05)
batch_size = 64
subset_size = len(X_train)

from collections import defaultdict

# Function to set random seeds
def set_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

# Number of runs to average
num_runs = 30
alpha_tests = []

# Initial Training (only once)
print("\nStarting Initial Training...\n")
model = CIFAR10ClassifierModule(num_classes=10)
model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

model = train_initial(
    model=model,
    optimizer=optimizer,
    criterion=criterion,
    train_data=train_data,
    val_data=val_data,
    test_data=test_dataset,
    init_epochs=init_epochs,
    batch_size=batch_size,
    device=device
)

save_path = "pretrained.pth"
model.save_checkpoint(optimizer, epoch=init_epochs, path=save_path)

alphas = np.linspace(0,2,20)

for run in range(num_runs):
    print(f"\nRun {run + 1}/{num_runs}")
    seed = run
    set_seed(seed)

    alpha_test = []
    for alpha in range(alphas.shape[0]):

      print(f"\nalpha: {alphas[alpha]:.2f}")

      # Load pretrained model
      model.load_checkpoint(optimizer, save_path, device)
      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

      trainer_alpha = ActiveLearningTrainer(
          model=model,
          criterion=criterion,
          optimizer=optimizer,
          train_data=train_data,
          val_data=val_data,
          test_data=test_dataset,
          unlabeled_data=unlabeled_data,
          batch_size=batch_size,
          posterior="plugin",
          sampling_strategy="entropy-redundancy",
          num_cycles=num_cycles,
          epochs_per_cycle=epochs_per_cycle,
          num_samples_per_cycle=num_samples_per_cycle,
          subset_size=subset_size,
          pretrain=False,
          alpha=alphas[alpha],
          epoch_print=False
      )
      alpha_test.append(trainer_alpha.active_learning_training())
    alpha_tests.append(alpha_test)

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

print(alpha_test)

num_alphas = len(alpha_tests[0])
averaged_test_losses = []
averaged_test_accuracies = []
loss_confidence_50 = []
loss_confidence_95 = []
accuracy_confidence_50 = []
accuracy_confidence_95 = []

for alpha_idx in range(num_alphas):
    test_losses = [run[alpha_idx]["test_loss"] for run in alpha_tests]
    averaged_test_losses.append(np.mean(test_losses))
    loss_confidence_50.append(np.percentile(test_losses, [25, 75]))
    loss_confidence_95.append(np.percentile(test_losses, [2.5, 97.5]))

    test_accuracies = [run[alpha_idx]["test_accuracy"] for run in alpha_tests]
    averaged_test_accuracies.append(np.mean(test_accuracies))
    accuracy_confidence_50.append(np.percentile(test_accuracies, [25, 75]))
    accuracy_confidence_95.append(np.percentile(test_accuracies, [2.5, 97.5]))


loss_confidence_50 = np.array(loss_confidence_50)
loss_confidence_95 = np.array(loss_confidence_95)
accuracy_confidence_50 = np.array(accuracy_confidence_50)
accuracy_confidence_95 = np.array(accuracy_confidence_95)

plt.figure(figsize=(10, 6))
plt.plot(alphas, averaged_test_losses, marker='', linestyle='-', linewidth=2, label='Mean Test Loss')
plt.fill_between(alphas, loss_confidence_50[:, 0], loss_confidence_50[:, 1], color='blue', alpha=0.2, label='50% Confidence Interval')
plt.fill_between(alphas, loss_confidence_95[:, 0], loss_confidence_95[:, 1], color='blue', alpha=0.1, label='95% Confidence Interval')
plt.title(f'Effect of Alpha on Test Loss - Averaged over {num_runs} runs', fontsize=14)
plt.xlabel('Alpha Values', fontsize=12)
plt.ylabel('Averaged Test Loss', fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.savefig('test_loss_vs_alpha.png', dpi=300, bbox_inches='tight')
plt.close()

plt.figure(figsize=(10, 6))
plt.plot(alphas, averaged_test_accuracies, marker='', linestyle='-', linewidth=2, label='Mean Test Accuracy')
plt.fill_between(alphas, accuracy_confidence_50[:, 0], accuracy_confidence_50[:, 1], color='green', alpha=0.2, label='50% Confidence Interval')
plt.fill_between(alphas, accuracy_confidence_95[:, 0], accuracy_confidence_95[:, 1], color='green', alpha=0.1, label='95% Confidence Interval')
plt.title(f'Effect of Alpha on Test Accuracy - Averaged over {num_runs} runs', fontsize=14)
plt.xlabel('Alpha Values', fontsize=12)
plt.ylabel('Averaged Test Accuracy', fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.savefig('test_accuracy_vs_alpha.png', dpi=300, bbox_inches='tight')
plt.close()

best_alpha_idx = np.argmin(averaged_test_losses)

best_alpha = alphas[best_alpha_idx]

model = CIFAR10ClassifierModule(num_classes=10)
criterion = nn.CrossEntropyLoss()
learning_rate = 0.0001
weight_decay = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Active Learning Parameters
num_cycles = 10
init_epochs = 40
epochs_per_cycle = 30
num_samples_per_cycle = int(initial_train_size*0.05)
batch_size = 64
subset_size = len(X_train)
alpha = best_alpha

from collections import defaultdict

# Function to set random seeds
def set_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

# Number of runs to average
num_runs = 30
metrics_random = []
metrics_plugin_entropy = []
metrics_plugin_entropy_redundancy = []
metrics_laplace_entropy = []
metrics_laplace_entropy_redundancy = []

# Initial Training (only once)
print("\nStarting Initial Training...\n")
model = CIFAR10ClassifierModule(num_classes=10)
model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

model = train_initial(
    model=model,
    optimizer=optimizer,
    criterion=criterion,
    train_data=train_data,
    val_data=val_data,
    test_data=test_dataset,
    init_epochs=init_epochs,
    batch_size=batch_size,
    device=device
)

save_path = "pretrained.pth"
model.save_checkpoint(optimizer, epoch=init_epochs, path=save_path)

for run in range(num_runs):
    print(f"\nRun {run + 1}/{num_runs}")
    seed = run
    set_seed(seed)

    # Load pretrained model
    model.load_checkpoint(optimizer, save_path, device)
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    # RANDOM SAMPLING
    print("\nRANDOM SAMPLING")
    trainer_random = ActiveLearningTrainer(
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        train_data=train_data,
        val_data=val_data,
        test_data=test_dataset,
        unlabeled_data=unlabeled_data,
        batch_size=batch_size,
        posterior="plugin",
        sampling_strategy="random",
        num_cycles=num_cycles,
        epochs_per_cycle=epochs_per_cycle,
        num_samples_per_cycle=num_samples_per_cycle,
        subset_size=subset_size,
        pretrain=False
    )
    metrics_random.append(trainer_random.active_learning_training())

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    model.load_checkpoint(optimizer, save_path, device)
    # PLUGIN-APPROXIMATION - Entropy (Default)
    print("\nPLUGIN-APPROXIMATION - Entropy (Default)")
    trainer_plugin = ActiveLearningTrainer(
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        train_data=train_data,
        val_data=val_data,
        test_data=test_dataset,
        unlabeled_data=unlabeled_data,
        batch_size=batch_size,
        posterior="plugin",
        sampling_strategy="entropy",
        num_cycles=num_cycles,
        epochs_per_cycle=epochs_per_cycle,
        num_samples_per_cycle=num_samples_per_cycle,
        subset_size=subset_size,
        pretrain=False,
        alpha=alpha,
        epoch_print=False
    )
    metrics_plugin_entropy.append(trainer_plugin.active_learning_training())

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    model.load_checkpoint(optimizer, save_path, device)

    # PLUGIN-APPROXIMATION - Entropy & Redundancy
    print("\nPLUGIN-APPROXIMATION - Entropy & Redundancy")
    trainer_plugin = ActiveLearningTrainer(
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        train_data=train_data,
        val_data=val_data,
        test_data=test_dataset,
        unlabeled_data=unlabeled_data,
        batch_size=batch_size,
        posterior="plugin",
        sampling_strategy="entropy-redundancy",
        num_cycles=num_cycles,
        epochs_per_cycle=epochs_per_cycle,
        num_samples_per_cycle=num_samples_per_cycle,
        subset_size=subset_size,
        pretrain=False,
        alpha=alpha,
        epoch_print=False
    )
    metrics_plugin_entropy_redundancy.append(trainer_plugin.active_learning_training())

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    model.load_checkpoint(optimizer, save_path, device)

    # PLUGIN-APPROXIMATION - Entropy & Redundancy
    print("\nLaplace Approximation - Entropy")
    trainer_plugin = ActiveLearningTrainer(
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        train_data=train_data,
        val_data=val_data,
        test_data=test_dataset,
        unlabeled_data=unlabeled_data,
        batch_size=batch_size,
        posterior="laplace",
        sampling_strategy="entropy",
        num_cycles=num_cycles,
        epochs_per_cycle=epochs_per_cycle,
        num_samples_per_cycle=num_samples_per_cycle,
        subset_size=subset_size,
        pretrain=False,
        alpha=alpha,
        epoch_print=False
    )
    metrics_laplace_entropy_redundancy.append(trainer_plugin.active_learning_training())

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

# Averaging results
def average_metrics(metrics_list):
    """
    Averages a list of metrics dictionaries, ensuring list values are averaged index-wise.
    """
    averaged_metrics = {}
    for key in metrics_list[0]:  # Iterate through keys in the first dictionary
        # Gather all values for the current key across all metrics
        values = [metrics[key] for metrics in metrics_list]

        # Handle list values by averaging element-wise (index-wise across metrics)
        if isinstance(values[0], list):
            max_len = max(len(v) for v in values)  # Determine the maximum length
            # Convert each list to float and pad shorter lists with NaNs
            padded_values = np.array([np.pad(np.array(v, dtype=float), (0, max_len - len(v)), constant_values=np.nan) for v in values])
            averaged_metrics[key] = np.nanmean(padded_values, axis=0).tolist()  # Average across rows (metrics)
        else:
            # Directly average scalar values
            averaged_metrics[key] = np.mean(values)
    return averaged_metrics

avg_metrics_random = average_metrics(metrics_random)
avg_metrics_plugin_entropy = average_metrics(metrics_plugin_entropy)
avg_metrics_plugin_entropy_redundancy = average_metrics(metrics_plugin_entropy_redundancy)
avg_metrics_laplace_entropy = average_metrics(metrics_laplace_entropy)
avg_metrics_laplace_entropy_redundancy = average_metrics(metrics_laplace_entropy_redundancy)

# @title Plot Averaged Metrics
def plot_metrics(metrics_list, labels, filename="metrics_plot.png"):
    fig, axs = plt.subplots(3, 2, figsize=(14, 18))  # 3 rows, 2 columns
    fig.tight_layout(pad=4.0)

    # Flatten the 2D array of axes for easier indexing
    axs = axs.flatten()

    # Iterate over each metrics dictionary and its label
    for metrics, label in zip(metrics_list, labels):
        axs[0].plot(metrics['train_sample_size'], metrics['train_loss'], label=f'{label}')
        axs[1].plot(metrics['train_sample_size'], metrics['val_loss'], label=f'{label}')
        axs[2].plot(metrics['train_sample_size'], metrics['val_accuracy'], label=f'{label}')
        axs[3].plot(metrics['train_sample_size'], metrics['test_loss'], label=f'{label}')
        axs[4].plot(metrics['train_sample_size'], metrics['test_accuracy'], label=f'{label}')
        axs[5].plot(metrics['train_sample_size'], metrics['average_entropy'], label=f'{label}')

    # Set titles, labels, and legends for subplots
    axs[0].set_xlabel('Training Set Size')
    axs[0].set_ylabel('Loss')
    axs[0].set_title('Training Loss')
    axs[0].grid(True)
    axs[0].legend()

    axs[1].set_xlabel('Training Set Size')
    axs[1].set_ylabel('Loss')
    axs[1].set_title('Validation Loss')
    axs[1].grid(True)
    axs[1].legend()

    axs[2].set_xlabel('Training Set Size')
    axs[2].set_ylabel('Accuracy')
    axs[2].set_title('Validation Accuracy')
    axs[2].grid(True)
    axs[2].legend()

    axs[3].set_xlabel('Training Set Size')
    axs[3].set_ylabel('Loss')
    axs[3].set_title('Test Loss')
    axs[3].grid(True)
    axs[3].legend()

    axs[4].set_xlabel('Training Set Size')
    axs[4].set_ylabel('Accuracy')
    axs[4].set_title('Test Accuracy')
    axs[4].grid(True)
    axs[4].legend()

    axs[5].set_xlabel('Training Set Size')
    axs[5].set_ylabel('Entropy')
    axs[5].set_title('Test Avg. Entropy')
    axs[5].grid(True)
    axs[5].legend()

    # Save the figure to a file
    fig.savefig(filename, bbox_inches='tight')

    # Close the figure to release memory
    plt.close(fig)

# Use averaged metrics for plotting
plot_metrics(
    [avg_metrics_laplace_entropy, avg_metrics_laplace_entropy_redundancy,avg_metrics_plugin_entropy, avg_metrics_plugin_entropy_redundancy, avg_metrics_random],
    ["Laplace Approximation - Entropy","Laplace Approximation - Entropy","Plugin-Approximation - Entropy", "Plugin-Approximation - Entropy & Redundancy", "Random Sampling"]
)

import csv

def save_dict_as_csv(dictionary, filename):
    # Open the file for writing
    with open(filename, mode='w', newline='') as file:
        writer = csv.writer(file)

        # Write the headers (keys of the dictionary)
        headers = list(dictionary.keys())
        writer.writerow(headers)

        # Determine the maximum length of values across all keys
        max_length = max(len(v) for v in dictionary.values())

        # Write the rows (values of the dictionary)
        for i in range(max_length):
            row = [dictionary[key][i] if i < len(dictionary[key]) else '' for key in dictionary]
            writer.writerow(row)

# Save the original metrics dictionaries
save_dict_as_csv(metrics_random, "metrics_random.csv")
save_dict_as_csv(metrics_plugin_entropy, "metrics_plugin_entropy.csv")
save_dict_as_csv(metrics_plugin_entropy_redundancy, "metrics_plugin_entropy_redundancy.csv")
save_dict_as_csv(metrics_laplace_entropy, "metrics_laplace_entropy.csv")
save_dict_as_csv(metrics_laplace_entropy_redundancy, "metrics_laplace_entropy_redundancy.csv")

# Save the averaged metrics dictionaries
save_dict_as_csv(avg_metrics_random, "avg_metrics_random.csv")
save_dict_as_csv(avg_metrics_plugin_entropy, "avg_metrics_plugin_entropy.csv")
save_dict_as_csv(avg_metrics_plugin_entropy_redundancy, "avg_metrics_plugin_entropy_redundancy.csv")
save_dict_as_csv(avg_metrics_laplace_entropy, "avg_metrics_laplace_entropy.csv")
save_dict_as_csv(avg_metrics_laplace_entropy_redundancy, "avg_metrics_laplace_entropy_redundancy.csv")